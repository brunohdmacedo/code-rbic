{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunohdmacedo/ML-FOR-DETECTION-OF-EXOPLANETS/blob/main/Data_Acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQDtyMM9zpLt"
      },
      "source": [
        "spogpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh2ewE9TztT8"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5S6bj6T4GD"
      },
      "source": [
        "!pip install lightkurve\n",
        "!pip install sktime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxLb0F8CbJR_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yun1NGQgbK8T"
      },
      "source": [
        "book_local = 'shallue_local_curves_'\n",
        "book_global = 'shallue_global_curves_'\n",
        "path_input = \"/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/lighkurve_KOI_dataset.csv\"\n",
        "path_local = '/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/Resultados teste/' + book_local + '.xlsx'\n",
        "path_global = '/content/drive/MyDrive/Iniciação Científica/IC_Exoplanetas_2022_Experimento/Base de Dados/Resultados teste/' + book_global + '.xlsx'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from lightkurve import search_lightcurve \n",
        "\n",
        "lc = pd.read_csv(path_input, sep = \",\") \n",
        "lc = lc[['kepid','koi_disposition','koi_period','koi_time0bk','koi_duration','koi_quarters']]\n",
        "\n",
        "lc.shape\n",
        "print('total inicial de curvas: %d\\n'%(lc.shape[0]))\n",
        "\n",
        "lc = lc.dropna()\n",
        "lc = lc[lc.koi_disposition != 'CANDIDATE']\n",
        "lc = lc.reset_index(drop=True)\n",
        "print('falsos positivos: %d, confirmados: %d\\n\\ntotal atualizado: %d\\n'%((lc.koi_disposition == 'FALSE POSITIVE').sum(),(lc.koi_disposition == 'CONFIRMED').sum(),lc.shape[0]))\n",
        "\n",
        "perc_class = ((lc.koi_disposition == 'FALSE POSITIVE').sum()*100)/lc.shape[0]\n",
        "print('falsos positivos: %.2f %% confirmados: %.2f %% \\n'%(perc_class,100-perc_class))\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "curvas_locais = []\n",
        "labels_locais = []\n",
        "curvas_globais = []\n",
        "labels_globais = []\n",
        "start_time = time.time()\n",
        "for index, row in lc[5000:6000].iterrows():\n",
        "  period, t0, duration_hours = row[2], row[3], row[4]\n",
        "\n",
        "  try:\n",
        "\n",
        "    lcs = search_lightcurve(str(row[0]), author='Kepler', cadence='long').download_all()\n",
        "\n",
        "    if (lcs != None):\n",
        "\n",
        "      lc_raw = lcs.stitch()\n",
        "      lc_raw.flux.shape\n",
        "\n",
        "      lc_clean = lc_raw.remove_outliers(sigma=3)\n",
        "\n",
        "      temp_fold = lc_clean.fold(period, epoch_time=t0)\n",
        "      fractional_duration = (duration_hours / 24.0) / period\n",
        "      phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
        "      transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
        "\n",
        "      lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
        "\n",
        "      lc_fold = lc_flat.fold(period, epoch_time=t0)\n",
        "\n",
        "      #global preprocessing-----------------------------------------------------\n",
        "      lc_global = lc_fold.bin(bins=2001).normalize() - 1\n",
        "      lc_global = (lc_global / np.abs(np.nanmin(lc_global.flux)) ) * 2.0 + 1\n",
        "      lc_global.flux.shape\n",
        "      #global preprocessing-----------------------------------------------------\n",
        "    \n",
        "      phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
        "      lc_zoom = lc_fold[phase_mask]\n",
        "\n",
        "      #local preprocessing------------------------------------------------------\n",
        "      lc_local = lc_zoom.bin(bins=201).normalize() - 1\n",
        "      lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
        "      lc_local.flux.shape\n",
        "      #local--------------------------------------------------------------------\n",
        "\n",
        "      labels_locais.append(row[1])\n",
        "      curvas_locais.append(lc_local.flux.value)\n",
        "\n",
        "      labels_globais.append(row[1])\n",
        "      curvas_globais.append(lc_global.flux.value)\n",
        "\n",
        "      print(index, 'OK')\n",
        "\n",
        "    else:\n",
        "      print(index, 'not downloaded')  \n",
        "    \n",
        "  except Exception as e:\n",
        "    print(index, e)\n",
        "\n",
        "t = time.time() - start_time   \n",
        "\n",
        "print('Tempo para importar curvas de luz: %f seconds\\n' %t)\n",
        "\n",
        "dataset_global = pd.DataFrame(curvas_globais)\n",
        "dataset_local = pd.DataFrame(curvas_locais)\n",
        "\n",
        "for i in range(1,len(curvas_globais)):\n",
        "    if len(curvas_globais[i]) != len(curvas_globais[i-1]):\n",
        "        print(\"A curva %d possui tamanho diferente das demais curvas. Tamanho: %d\"%(i,len(curvas_globais[i])))\n",
        "print(\"Caso nenhuma das curvas apresente tamanho diferente, todas as curvas GLOBAIS possuem o total de %d pontos cada.\\n\"%len(curvas_globais[0]))\n",
        "\n",
        "for i in range(1,len(curvas_locais)):\n",
        "    if len(curvas_locais[i]) != len(curvas_locais[i-1]):\n",
        "        print(\"A curva %d possui tamanho diferente das demais curvas. Tamanho: %d\"%(i,len(curvas_locais[i])))\n",
        "print(\"Caso nenhuma das curvas apresente tamanho diferente, todas as curvas LOCAIS possuem o total de %d pontos cada.\\n\"%len(curvas_locais[0]))\n",
        "\n",
        "print(\"Quantidade de NaN na base GLOBAL: %s\"%dataset_global.isna().sum(axis=1).sum())\n",
        "print(\"Quantidade de NaN na base LOCAL: %s\\n\"%dataset_local.isna().sum(axis=1).sum())\n",
        "\n",
        "perc_nan_glob = (dataset_global.isna().sum(axis=1).sum()*100)/dataset_global.count(axis=1).sum()\n",
        "print(\"Porcentagem de valores do dataset GLOBAL substituídos na interpolação: %.2f %%\"%perc_nan_glob)\n",
        "perc_nan_loc = (dataset_local.isna().sum(axis=1).sum()*100)/dataset_local.count(axis=1).sum()\n",
        "print(\"Porcentagem de valores do dataset LOCAL substituídos na interpolação: %.2f %%\"%perc_nan_loc)\n",
        "\n",
        "dataset_global = dataset_global.interpolate(axis=1)\n",
        "dataset_local = dataset_local.interpolate(axis=1)\n",
        "\n",
        "print(\"Quantidade de NaN na base GLOBAL após interpolação: %s --> deve sempre ser zero\"%dataset_global.isna().sum(axis=1).sum())\n",
        "print(\"Quantidade de NaN na base LOCAL após interpolação: %s --> deve sempre ser zero\"%dataset_local.isna().sum(axis=1).sum())\n",
        "\n",
        "labels_glob = pd.Series(labels_globais)\n",
        "labels_loc = pd.Series(labels_locais)\n",
        "dataset_global['label'] = labels_glob\n",
        "dataset_local['label'] = labels_loc\n",
        "\n",
        "dataset_global.to_csv(path_global,index=False)  \n",
        "dataset_local.to_csv(path_local,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}